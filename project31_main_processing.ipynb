{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# NEWSPAPER TITLE CLASSIFICATION BASED ON KNN, KMEANS AND DECISION TREE",
   "id": "2132299393a96450"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To have general view & data structure of Project, refer the `Readme.md` of this Project and general structure of project",
   "id": "42662c5fddf4973b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![General Structure](general_structure.png)",
   "id": "59becc4a75a05a3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. IMPORT LIBRARY",
   "id": "a79b7755daedafa"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:20.209665Z",
     "start_time": "2025-08-22T06:46:20.207347Z"
    }
   },
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# from chromadb.api.types import normalize_embeddings\n",
    "from langchain.evaluation import load_dataset\n",
    "# from sentence_transformers import SentenceTransformer"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:20.236672Z",
     "start_time": "2025-08-22T06:46:20.234382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import nltk # use in case\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Literal, List, Union, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch"
   ],
   "id": "690c66133f95e2eb",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:20.285411Z",
     "start_time": "2025-08-22T06:46:20.282337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix, silhouette_score"
   ],
   "id": "b2fc68d6fc60363",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. DATA FEATURE EXTRACTION",
   "id": "f743167a136c2f41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`Note: Refer the Readme.md for Data source collection`",
   "id": "ca1de0a7af862900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:24.630990Z",
     "start_time": "2025-08-22T06:46:20.331943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('UniverseTBD/arxiv-abstracts-large')"
   ],
   "id": "6194995cd948e0c8",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dataset is dictionary collection with feature fields",
   "id": "6537d73882e883b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:24.693564Z",
     "start_time": "2025-08-22T06:46:24.688914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(type(ds))\n",
    "ds"
   ],
   "id": "6b8981d09ac5ba5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed'],\n",
       "        num_rows: 2292057\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> Find the path of the data files:\n",
    "\n",
    "The data has been downloaded then stored in the path (with Linux)\n",
    "\"/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/*.arrow\"\n",
    "\n",
    "During loading wiht dataset, the command load_dataset shall be removed to cancel the download program"
   ],
   "id": "f627a43de5f7a8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:24.750188Z",
     "start_time": "2025-08-22T06:46:24.746845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# list down the path of data file\n",
    "print(ds.cache_files)"
   ],
   "id": "e0485d0117a9c7c4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [{'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00000-of-00007.arrow'}, {'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00001-of-00007.arrow'}, {'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00002-of-00007.arrow'}, {'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00003-of-00007.arrow'}, {'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00004-of-00007.arrow'}, {'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00005-of-00007.arrow'}, {'filename': '/home/anhvt/.cache/huggingface/datasets/UniverseTBD___arxiv-abstracts-large/default/0.0.0/6020a62078a73d7ca02b86a4a775af7caba42d5e/arxiv-abstracts-large-train-00006-of-00007.arrow'}]}\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Collect the fields by keys of dict dataset",
   "id": "4ab236ae079b822d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:24.808725Z",
     "start_time": "2025-08-22T06:46:24.803797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_ds = ds['train']\n",
    "topic_features = train_ds.column_names\n",
    "print(topic_features)"
   ],
   "id": "5e73a37af04572cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'submitter', 'authors', 'title', 'comments', 'journal-ref', 'doi', 'report-no', 'categories', 'license', 'abstract', 'versions', 'update_date', 'authors_parsed']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Following the guidelines of Project, the data will use 'abstract' as input or features and 'categories' as labels.\n",
    "\n",
    "For categories, the value is separate into 02 fields: main category and sub category, in example: [math.CA] [cs.CG]\n",
    "\n",
    "We shall need to extract primary category for this Project then lower the letters"
   ],
   "id": "2454f315ee039720"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:26.736300Z",
     "start_time": "2025-08-22T06:46:24.858173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_splitted = train_ds.select_columns(['abstract', 'categories'])\n",
    "project_df = ds_splitted.to_pandas()"
   ],
   "id": "153efc909f01f1fc",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:29.497532Z",
     "start_time": "2025-08-22T06:46:26.796334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# separate the value to 2-dimension list: 'acc.phy math' => [acc.phy, math] => acc\n",
    "category = project_df['categories'].map(lambda x: x.split(' '))\n",
    "category = category.map(lambda x: x[0].split('.')[0])\n",
    "\n",
    "category_set = set(category)\n",
    "print(f'Length of unique primary categories is {len(category_set)}')"
   ],
   "id": "deb564e22351e6ba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of unique primary categories is 38\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The requirement is extract 1000-2000 Data values base on primary categories below:\n",
    "\n",
    "`[astro-ph, cond-mat, cs, math, physics]`"
   ],
   "id": "8caa10f77be83d5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.285495Z",
     "start_time": "2025-08-22T06:46:29.635319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "use_categories_list = ['astro-ph', 'cond-mat', 'cs', 'math', 'physics']\n",
    "\n",
    "# creat Regrex OR by '|'\n",
    "# 'astro-ph'|'cond-mat'|'cs'|'math'|'physics'\n",
    "pattern = '^(' + '|'.join(use_categories_list) + ')'\n",
    "# filter the data\n",
    "project_df_filtered = project_df[project_df['categories'].str.contains(pattern, case=False, na=False)]\n",
    "# extract 2000 values in random list\n",
    "dataset_df = project_df_filtered.sample(n=2000, random_state=42)\n",
    "dataset_df.reset_index(drop=True, inplace=True)\n",
    "dataset_df.head(10)"
   ],
   "id": "e8854a26b1ec662d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_142911/728025914.py:7: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  project_df_filtered = project_df[project_df['categories'].str.contains(pattern, case=False, na=False)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                            abstract  \\\n",
       "0    The factorially normalized Bernoulli polynom...   \n",
       "1    We propose a simple uniform lower bound on t...   \n",
       "2    We study truncated point schemes of connecte...   \n",
       "3    Four-dimensional (4D) printing, a new techno...   \n",
       "4    We show that the d-th secant variety of a pr...   \n",
       "5    In recent years, badminton analytics has dra...   \n",
       "6    The influence that the kinematics of pitchin...   \n",
       "7    We address the problem of evaluating the tra...   \n",
       "8    In this paper, we derive the large deviation...   \n",
       "9    The Three-Body Parameter(3BP)\\n$a^{\\scriptsc...   \n",
       "\n",
       "                                          categories  \n",
       "0                                    math.PR math.CO  \n",
       "1                                            math.CA  \n",
       "2                            math.RA math.AG math.QA  \n",
       "3                               cs.CG physics.app-ph  \n",
       "4                                    math.AG math.NT  \n",
       "5                                              cs.AI  \n",
       "6                                    physics.flu-dyn  \n",
       "7                  physics.bio-ph cond-mat.stat-mech  \n",
       "8                            math.PR math-ph math.MP  \n",
       "9  cond-mat.quant-gas physics.atm-clus physics.at...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The factorially normalized Bernoulli polynom...</td>\n",
       "      <td>math.PR math.CO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We propose a simple uniform lower bound on t...</td>\n",
       "      <td>math.CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We study truncated point schemes of connecte...</td>\n",
       "      <td>math.RA math.AG math.QA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Four-dimensional (4D) printing, a new techno...</td>\n",
       "      <td>cs.CG physics.app-ph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We show that the d-th secant variety of a pr...</td>\n",
       "      <td>math.AG math.NT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>In recent years, badminton analytics has dra...</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The influence that the kinematics of pitchin...</td>\n",
       "      <td>physics.flu-dyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We address the problem of evaluating the tra...</td>\n",
       "      <td>physics.bio-ph cond-mat.stat-mech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In this paper, we derive the large deviation...</td>\n",
       "      <td>math.PR math-ph math.MP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Three-Body Parameter(3BP)\\n$a^{\\scriptsc...</td>\n",
       "      <td>cond-mat.quant-gas physics.atm-clus physics.at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. DATA PROCESSING",
   "id": "366c99e1f9dc1987"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.369212Z",
     "start_time": "2025-08-22T06:46:30.365749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check one value of dataset\n",
    "dataset_df.loc[1, 'abstract']"
   ],
   "id": "c24ff8873c6a1cf4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  We propose a simple uniform lower bound on the spacings between the\\nsuccessive zeros of the Laguerre polynomials $L_n^{(\\\\alpha)}$ for all\\n$\\\\alpha>-1$. Our bound is sharp regarding the order of dependency on $n$ and\\n$\\\\alpha$ in various ranges. In particular, we recover the orders given in\\n\\\\cite{ahmed} for $\\\\alpha \\\\in (-1,1]$.\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Data Processing will perform activities below to create raw values:\n",
    "* Removes `\\n` and whitespace characters at the beginning and end of the string.\n",
    "* Removes special characters (punctuation, non-letter or numeric characters).\n",
    "* Removes digits.\n",
    "* Converts all letters to lowercase.\n",
    "* Gets the label as the primary category (first part) in the categories field."
   ],
   "id": "aed343d8efdf4687"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.509767Z",
     "start_time": "2025-08-22T06:46:30.507091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def abstract_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess abstracts: remove all special characters\n",
    "    :param abstract_text: the text to be preprocessed\n",
    "    :return: text after preprocessing\n",
    "    \"\"\"\n",
    "    # remove the enter space\n",
    "    text = text.strip().replace('\\n', ' ')\n",
    "    # remove the special letters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # remove digit\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # lower case\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def category_processing(text):\n",
    "    \"\"\"\n",
    "    Function to preprocess categories: collect only first part\n",
    "    :param text: catergories to be processed\n",
    "    :return: text after preprocessing\n",
    "    \"\"\"\n",
    "    text_splitted = text.replace('.',' ')\n",
    "    text_category = text_splitted.split(' ')[0]\n",
    "    return text_category"
   ],
   "id": "58752fefc819cbd5",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test function of processing",
   "id": "83ad831ebc491674"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.561246Z",
     "start_time": "2025-08-22T06:46:30.558702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a_sample = dataset_df.loc[1, 'abstract']\n",
    "c_sample = dataset_df.loc[6, 'categories']\n",
    "print('before:\\n', a_sample)\n",
    "print('after: \\n', abstract_preprocessing(a_sample))\n",
    "print('before:\\n', c_sample)\n",
    "print('after: \\n', category_processing(c_sample))"
   ],
   "id": "927bf4a471adff0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before:\n",
      "   We propose a simple uniform lower bound on the spacings between the\n",
      "successive zeros of the Laguerre polynomials $L_n^{(\\alpha)}$ for all\n",
      "$\\alpha>-1$. Our bound is sharp regarding the order of dependency on $n$ and\n",
      "$\\alpha$ in various ranges. In particular, we recover the orders given in\n",
      "\\cite{ahmed} for $\\alpha \\in (-1,1]$.\n",
      "\n",
      "after: \n",
      " we propose a simple uniform lower bound on the spacings between the successive zeros of the laguerre polynomials l_nalpha for all alpha our bound is sharp regarding the order of dependency on n and alpha in various ranges in particular we recover the orders given in citeahmed for alpha in\n",
      "before:\n",
      " physics.flu-dyn\n",
      "after: \n",
      " physics\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function is working correctly. Apply to all dataset",
   "id": "b391270ff8ed42f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.688206Z",
     "start_time": "2025-08-22T06:46:30.609849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_df = dataset_df.assign(\n",
    "    abstract = dataset_df['abstract'].apply(abstract_preprocessing),\n",
    "    categories = dataset_df['categories'].apply(category_processing)\n",
    ")"
   ],
   "id": "76e24ac03c7729a1",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.714902Z",
     "start_time": "2025-08-22T06:46:30.702383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(dataset_df.info())\n",
    "dataset_df.head(5)"
   ],
   "id": "e412b556785cbb57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   abstract    2000 non-null   object\n",
      " 1   categories  2000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 31.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                            abstract categories\n",
       "0  the factorially normalized bernoulli polynomia...       math\n",
       "1  we propose a simple uniform lower bound on the...       math\n",
       "2  we study truncated point schemes of connected ...       math\n",
       "3  fourdimensional d printing a new technology em...         cs\n",
       "4  we show that the dth secant variety of a proje...       math"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the factorially normalized bernoulli polynomia...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we propose a simple uniform lower bound on the...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we study truncated point schemes of connected ...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fourdimensional d printing a new technology em...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we show that the dth secant variety of a proje...</td>\n",
       "      <td>math</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.838854Z",
     "start_time": "2025-08-22T06:46:30.832229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check categories unique value\n",
    "print(sorted(dataset_df['categories'].unique()))\n",
    "sorted_catergories_list = sorted(dataset_df['categories'].unique())\n",
    "sorted_catergories_list.index('math-ph')"
   ],
   "id": "bc90f90d1a6d4aa6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astro-ph', 'cond-mat', 'cs', 'math', 'math-ph', 'physics']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert categories to number",
   "id": "6f11f715a30c7d09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:30.978888Z",
     "start_time": "2025-08-22T06:46:30.973795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_df['categories'] = dataset_df['categories'].map(lambda x: sorted_catergories_list.index(x))\n",
    "dataset_df.head(5)"
   ],
   "id": "3bf37446c2ac470e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            abstract  categories\n",
       "0  the factorially normalized bernoulli polynomia...           3\n",
       "1  we propose a simple uniform lower bound on the...           3\n",
       "2  we study truncated point schemes of connected ...           3\n",
       "3  fourdimensional d printing a new technology em...           2\n",
       "4  we show that the dth secant variety of a proje...           3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the factorially normalized bernoulli polynomia...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we propose a simple uniform lower bound on the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we study truncated point schemes of connected ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fourdimensional d printing a new technology em...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we show that the dth secant variety of a proje...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. DATA EMBEDDING",
   "id": "57acb765e5270c5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We shall apply 03 embedding method for text: CountVectorizer(), tfidf_vectorizer(), embedding_vectorizer()",
   "id": "dcbedb3cb66fc251"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:31.128724Z",
     "start_time": "2025-08-22T06:46:31.121744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset_df['abstract'], dataset_df['categories'], test_size=0.2, random_state=42)\n",
    "print(f'Training sample: {len(X_train)}')\n",
    "print(f'Test sample: {len(X_test)}')\n",
    "print(f'Training labels: {len(y_train)}')\n",
    "print(f'Test labels: {len(y_test)}')"
   ],
   "id": "e28de66f50859426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sample: 1600\n",
      "Test sample: 400\n",
      "Training labels: 1600\n",
      "Test labels: 400\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### This model does not have normalization",
   "id": "a5f63d51be85239f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:31.355845Z",
     "start_time": "2025-08-22T06:46:31.251299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "print(X_train_bow.shape)\n",
    "print(X_test_bow.shape)\n",
    "print(X_train_bow[0])"
   ],
   "id": "294e853df777078d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 18450)\n",
      "(400, 18450)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
      "\twith 102 stored elements and shape (1, 18450)>\n",
      "  Coords\tValues\n",
      "  (0, 14782)\t2\n",
      "  (0, 9967)\t1\n",
      "  (0, 6916)\t1\n",
      "  (0, 1327)\t2\n",
      "  (0, 12956)\t1\n",
      "  (0, 16836)\t8\n",
      "  (0, 5218)\t1\n",
      "  (0, 16616)\t8\n",
      "  (0, 11265)\t4\n",
      "  (0, 11383)\t9\n",
      "  (0, 2535)\t7\n",
      "  (0, 7620)\t5\n",
      "  (0, 3661)\t4\n",
      "  (0, 1267)\t1\n",
      "  (0, 7449)\t1\n",
      "  (0, 1341)\t1\n",
      "  (0, 433)\t1\n",
      "  (0, 16618)\t1\n",
      "  (0, 6897)\t1\n",
      "  (0, 15868)\t1\n",
      "  (0, 562)\t2\n",
      "  (0, 7699)\t2\n",
      "  (0, 16611)\t2\n",
      "  (0, 9840)\t1\n",
      "  (0, 7976)\t1\n",
      "  :\t:\n",
      "  (0, 18078)\t1\n",
      "  (0, 5988)\t1\n",
      "  (0, 11185)\t1\n",
      "  (0, 15236)\t1\n",
      "  (0, 5800)\t2\n",
      "  (0, 5219)\t1\n",
      "  (0, 4021)\t1\n",
      "  (0, 9896)\t1\n",
      "  (0, 12855)\t1\n",
      "  (0, 5417)\t1\n",
      "  (0, 12083)\t1\n",
      "  (0, 18156)\t2\n",
      "  (0, 8721)\t1\n",
      "  (0, 4183)\t1\n",
      "  (0, 16303)\t1\n",
      "  (0, 13504)\t1\n",
      "  (0, 3662)\t1\n",
      "  (0, 14893)\t1\n",
      "  (0, 6563)\t1\n",
      "  (0, 13993)\t1\n",
      "  (0, 14180)\t1\n",
      "  (0, 10969)\t1\n",
      "  (0, 3919)\t1\n",
      "  (0, 17670)\t1\n",
      "  (0, 18159)\t1\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### This model has default L2 normalization",
   "id": "3e74a4874b683ec0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:31.481725Z",
     "start_time": "2025-08-22T06:46:31.380530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(X_train_tfidf[0])"
   ],
   "id": "5064ba946619af4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 18450)\n",
      "(400, 18450)\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 102 stored elements and shape (1, 18450)>\n",
      "  Coords\tValues\n",
      "  (0, 14782)\t0.09969270018514081\n",
      "  (0, 9967)\t0.04335014971731978\n",
      "  (0, 6916)\t0.03388746193404882\n",
      "  (0, 1327)\t0.0831834845247899\n",
      "  (0, 12956)\t0.043939168180104515\n",
      "  (0, 16836)\t0.12914015150150226\n",
      "  (0, 5218)\t0.05746113414142209\n",
      "  (0, 16616)\t0.11284799955964703\n",
      "  (0, 11265)\t0.1742724798821033\n",
      "  (0, 11383)\t0.12703332883853058\n",
      "  (0, 2535)\t0.4519794853757873\n",
      "  (0, 7620)\t0.07723101105628082\n",
      "  (0, 3661)\t0.24326615353579287\n",
      "  (0, 1267)\t0.07020992990846252\n",
      "  (0, 7449)\t0.07235470281307164\n",
      "  (0, 1341)\t0.07635736251478561\n",
      "  (0, 433)\t0.040551569206818947\n",
      "  (0, 16618)\t0.05631691573756624\n",
      "  (0, 6897)\t0.03526844413282702\n",
      "  (0, 15868)\t0.037494109609882406\n",
      "  (0, 562)\t0.04664977637875393\n",
      "  (0, 7699)\t0.13501708690010744\n",
      "  (0, 16611)\t0.037943998999576\n",
      "  (0, 9840)\t0.07124102993908804\n",
      "  (0, 7976)\t0.10128697791112083\n",
      "  :\t:\n",
      "  (0, 18078)\t0.06792390215335287\n",
      "  (0, 5988)\t0.018088114471988537\n",
      "  (0, 11185)\t0.03662279965895064\n",
      "  (0, 15236)\t0.05411993457932174\n",
      "  (0, 5800)\t0.13944337483479552\n",
      "  (0, 5219)\t0.0687937780941552\n",
      "  (0, 4021)\t0.06490377879153456\n",
      "  (0, 9896)\t0.09728431820940686\n",
      "  (0, 12855)\t0.06524733904366699\n",
      "  (0, 5417)\t0.04863571875465341\n",
      "  (0, 12083)\t0.060565838209112786\n",
      "  (0, 18156)\t0.03914461424503252\n",
      "  (0, 8721)\t0.0404927383231054\n",
      "  (0, 4183)\t0.06360856674774519\n",
      "  (0, 16303)\t0.07294685529620265\n",
      "  (0, 13504)\t0.057069179116086145\n",
      "  (0, 3662)\t0.05892706591319099\n",
      "  (0, 14893)\t0.03213956371975093\n",
      "  (0, 6563)\t0.056687963947144926\n",
      "  (0, 13993)\t0.03220412756306995\n",
      "  (0, 14180)\t0.0687937780941552\n",
      "  (0, 10969)\t0.0615969382397383\n",
      "  (0, 3919)\t0.046400890627765055\n",
      "  (0, 17670)\t0.0687937780941552\n",
      "  (0, 18159)\t0.048955583031824325\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Buld class of user-define vector embedding\n",
    "The Sentence Transformer is not compatible with Python. The solution is using the torch and hugging faces transformer then auto-pooling the text\n",
    "Embedding Vectorizer in this case shall use:\n",
    "* HuggingFace Embedding\n",
    "* CUDA with auto transform to GPU/ CPU\n",
    "* Batch-size with long/ complicate input"
   ],
   "id": "2d83554f5c37f2c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:31.497860Z",
     "start_time": "2025-08-22T06:46:31.492774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EmbeddingVectorizer:\n",
    "    \"\"\"\n",
    "    Vectorizer use SentenceTransformers (default: intfloat/multilingual-e5-base).\n",
    "    - mode='query'   -> prefix \"query: \"\n",
    "    - mode='passage' -> prefix \"passage: \"\n",
    "    - mode='raw'     -> keep raw text\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 model_name: str = 'intfloat/multilingual-e5-base',\n",
    "                 normalize: bool = True,\n",
    "                 device: Literal['auto', 'cpu', 'cuda'] = 'auto'):\n",
    "        # specify the device\n",
    "        if device == 'auto':\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.device = device\n",
    "\n",
    "        print(f\"[EmbeddingVectorizer] Using device: {self.device}\")\n",
    "\n",
    "        # load model/tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def _pool_embeddings(self, model_output):\n",
    "        # mean pooling sBERT style\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        attention_mask = model_output.attention_mask\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)\n",
    "        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)\n",
    "        return sum_embeddings / sum_mask\n",
    "\n",
    "    def _format_inputs(self, texts: List[str], mode: Literal['query', 'passage', 'raw'] = 'query'):\n",
    "        if mode == 'query':\n",
    "            return [f\"query: {t}\" for t in texts]\n",
    "        elif mode == 'passage':\n",
    "            return [f\"passage: {t}\" for t in texts]\n",
    "        return texts\n",
    "\n",
    "    def transform_to_numpy(self,\n",
    "                           texts: List[str],\n",
    "                           mode: Literal['query', 'passage', 'raw'] = 'query',\n",
    "                           batch_size: int = 16):\n",
    "        \"\"\"\n",
    "        Encode 1 list văn bản -> numpy array [n_samples, embedding_dim]\n",
    "        batch_size: batch encoding to use GPU better\n",
    "        \"\"\"\n",
    "        inputs = self._format_inputs(texts, mode)\n",
    "        all_embeddings = []\n",
    "\n",
    "        for i in range(0, len(inputs), batch_size):\n",
    "            batch = inputs[i:i + batch_size]\n",
    "            encoded = self.tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**encoded)\n",
    "            embs = self._pool_embeddings(outputs)\n",
    "\n",
    "            if self.normalize:\n",
    "                embs = torch.nn.functional.normalize(embs)\n",
    "\n",
    "            all_embeddings.append(embs.cpu())\n",
    "\n",
    "        all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        return all_embeddings.numpy()"
   ],
   "id": "6f744eae7767f699",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Test embedding class, version of SentenceTransformer not compatible with Python",
   "id": "3c824ac876933900"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:31.544181Z",
     "start_time": "2025-08-22T06:46:31.542335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# embedding_vectorizer = EmbeddingVectorizer()\n",
    "# print(embedding_vectorizer.transform([a_sample]))"
   ],
   "id": "9c16cef6d328a95b",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. PRODUCTION MODEL\n",
    "* KMean Clustering\n",
    "* KNN\n",
    "* Decision Tree\n",
    "\n",
    "Một câu hỏi thực tế, đối với các dữ liệu nhúng, Data Embedding, có cần phải chuẩn hóa dữ liệu không:\n",
    "* Nếu chỉ dùng embedding để so sánh độ tương tự → dùng L2 normalization.\n",
    "* Nếu đưa embedding vào machine learning model (LR, SVM, KMeans, KNN) → thường nên chuẩn hóa (StandardScaler hoặc MinMaxScaler).\n",
    "* Nếu embedding đã được normalize từ trước → không cần làm thêm.\n",
    "\n",
    "`Embedding thường đã chuẩn hóa sẵn (normalized)`\n",
    "\n",
    "Các mô hình này thường trả về vector đã L2-normalized (độ dài = 1) hoặc cho phép bật tùy chọn normalize:\n",
    "* SentenceTransformers (khi dùng normalize_embeddings=True)\n",
    "* OpenAI Embeddings (ví dụ text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large) → mặc định đã chuẩn hóa.\n",
    "* Google Universal Sentence Encoder (USE) → chuẩn hóa sẵn.\n",
    "* Cohere Embeddings API → chuẩn hóa sẵn.\n",
    "* FastText pre-trained vectors (thường normalize trong nhiều thư viện ứng dụng).\n",
    "* Word2Vec/GloVe nếu bạn tự normalize sau khi load (thư viện Gensim hay normalize theo option).\n",
    "* 👉 Các embedding này dùng cosine similarity hoặc dot product trực tiếp mà không cần scaling thêm.\n",
    "\n",
    "`Embedding chưa chuẩn hóa (raw vector)`\n",
    "\n",
    "Các mô hình này trả về vector thô, độ dài khác nhau tùy câu/từ:\n",
    "\n",
    "* BERT / RoBERTa / DistilBERT raw embeddings (hidden states).\n",
    "* GPT hidden states khi lấy từ transformer layers.\n",
    "* CLIP (raw text/image embeddings) → cần normalize trước khi tính similarity cosine.\n",
    "* E5 models (intfloat/e5-base, multilingual-e5-base) → chưa normalize mặc định, nhưng SentenceTransformers wrapper thường cho phép bật normalize.\n",
    "* Transformer-based encoder khác (XLM-R, mBERT, LaBSE) nếu lấy trực tiếp từ HuggingFace mà không normalize.\n",
    "* Custom CNN/RNN embeddings (khi bạn huấn luyện encoder riêng)."
   ],
   "id": "21b00fbfbf57c6f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5.1 KMEANS\n",
    "Silhouetes is use for this option"
   ],
   "id": "b3d7fd3c8df1272e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:31.596683Z",
     "start_time": "2025-08-22T06:46:31.591529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_test_kmean(X_train, X_test, y_train, y_test, cluster_range: int=3):\n",
    "    \"\"\"\n",
    "    This function use KMEAN for predict the title. The cluster (k) is apply from 1-30. However to determine the best k, the silhouette method is used\n",
    "    :param X_train: the training set, scaled to sparse matrix\n",
    "    :param X_test: the test set, scaled to sparse matrix\n",
    "    :param y_train: the train set, int value\n",
    "    :param y_test: the test set, int value\n",
    "    :param cluster_range: the range of cluster:\n",
    "    :return: predict value y_pred, accuracy, report\n",
    "    \"\"\"\n",
    "    # Find the optimum cluster number k\n",
    "    silhouettes = {}\n",
    "    if cluster_range < 3:\n",
    "        print('number of clusters should be greater than 3')\n",
    "        return None\n",
    "    else:\n",
    "        for i in range(2, cluster_range):\n",
    "            kmeans = KMeans(n_clusters=i, random_state=42).fit(X_train)\n",
    "            silhouettes[i] = silhouette_score(X_train, kmeans.labels_)\n",
    "\n",
    "        max_si_value_key = max(silhouettes, key=silhouettes.get)\n",
    "        silhouette_final_value = max_si_value_key + 1 # Get the optimum near max silhouette value\n",
    "\n",
    "        kmeans = KMeans(n_clusters=silhouette_final_value, random_state=42)\n",
    "        kmeans.fit(X_train)\n",
    "        y_pred = kmeans.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        return y_pred, accuracy, report, silhouette_final_value"
   ],
   "id": "a49e6c289943c782",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:33.228153Z",
     "start_time": "2025-08-22T06:46:31.645075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kmean_bow_labels, kmean_bow_accurracy, kmean_bow_report, bow_k = train_and_test_kmean(X_train_bow, X_test_bow, y_train, y_test, 10)\n",
    "kmean_tfidf_labels, kmeans_tfidf_accuracy, kmean_tfidf_report, tfidf_k = train_and_test_kmean(X_train_tfidf, X_test_tfidf, y_train, y_test, 10)\n",
    "\n",
    "print(\"Accuracies for K-Means:\")\n",
    "print(f\"Bag of Words: {kmean_bow_accurracy:.4f} with {bow_k} clusters\")\n",
    "print(f\"Tf-Idf: {kmeans_tfidf_accuracy:.4f} with {tfidf_k} clusters\")"
   ],
   "id": "2ddf7b4a79f01d60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for K-Means:\n",
      "Bag of Words: 0.2800 with 3 clusters\n",
      "Tf-Idf: 0.3750 with 3 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.2 KNN",
   "id": "ac3d84ff844d2c90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:33.293686Z",
     "start_time": "2025-08-22T06:46:33.290550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_test_knn(X_train, X_test, y_train, y_test, n_neighbors: int = 5):\n",
    "    \"\"\"\n",
    "    The knn function to predict the categories\n",
    "    :param X_train:\n",
    "    :param X_test:\n",
    "    :param y_train:\n",
    "    :param y_test:\n",
    "    :param n_neighbors:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "     # Predict on the test set\n",
    "\n",
    "    y_pred = knn.predict(X_test)\n",
    "\n",
    "    # Calculate accuracy and classification report\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return y_pred, accuracy, report"
   ],
   "id": "e2cd0b0cb5bedfe0",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T06:46:37.791616Z",
     "start_time": "2025-08-22T06:46:37.737953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "knn_bow_labels, knn_bow_accuracy, knn_bow_report = train_and_test_knn(X_train_bow, X_test_bow, y_train, y_test, 5)\n",
    "knn_tfidf_labels, knn_tfidf_accuracy, knn_tfidf_report = train_and_test_knn(X_train_tfidf, X_test_tfidf, y_train, y_test, 5)\n",
    "\n",
    "print(\"Accuracies for KNN:\")\n",
    "print(f\"Bag of Words: {knn_bow_accuracy:.4f} with 5 neighbors\")\n",
    "print(f\"Tf-Idf: {knn_tfidf_accuracy:.4f} with 5 neighbors\")"
   ],
   "id": "a5e575959120e0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for KNN:\n",
      "Bag of Words: 0.3825 with 5 neighbors\n",
      "Tf-Idf: 0.7875 with 5 neighbors\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 5.3 DECISION TREE",
   "id": "a9af1b02dd7242d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T13:08:33.426053Z",
     "start_time": "2025-08-22T13:08:33.422162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_test_decision_tree(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    The decision tree function to predict the categories\n",
    "    :param X_train:\n",
    "    :param y_train:\n",
    "    :param X_test:\n",
    "    :param y_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dt = DecisionTreeClassifier(max_depth=5)\n",
    "    dt.fit(X_train, y_train)\n",
    "    # Predict on the test set\n",
    "    y_pred = dt.predict(X_test)\n",
    "    # Calculate accuracy and classification report\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    return y_pred, accuracy, report"
   ],
   "id": "d7dd35e412f99b74",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-22T13:09:04.578926Z",
     "start_time": "2025-08-22T13:09:04.448725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dt_bow_labels, dt_bow_accuracy, dt_bow_report = train_and_test_decision_tree(X_train_bow, X_test_bow, y_train, y_test)\n",
    "dt_tfidf_labels, dt_tfidf_accuracy, dt_tfidf_report = train_and_test_decision_tree(X_train_tfidf, X_test_tfidf, y_train, y_test)\n",
    "\n",
    "print(\"Accuracies for Decision Tree:\")\n",
    "print(f\"Bag of Words: {dt_bow_accuracy:.4f}\")\n",
    "print(f\"Tf-Idf: {dt_tfidf_accuracy:.4f}\")"
   ],
   "id": "b1092ae299f8e46c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracies for Decision Tree:\n",
      "Bag of Words: 0.4800\n",
      "Tf-Idf: 0.4800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/media/anhvt/DATA/_PyLIB_LINUX/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3001143c4af4bc9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
